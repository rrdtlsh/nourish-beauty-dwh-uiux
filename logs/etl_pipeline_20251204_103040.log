2025-12-04 10:30:40,276 - __main__ - INFO - ================================================================================
2025-12-04 10:30:40,276 - __main__ - INFO - NOURISH BEAUTY DATA WAREHOUSE - ETL PIPELINE
2025-12-04 10:30:40,276 - __main__ - INFO -    Author: Raudatul Sholehah - 2310817220002
2025-12-04 10:30:40,276 - __main__ - INFO -    Date: 2025-12-04 10:30:40
2025-12-04 10:30:40,276 - __main__ - INFO - ================================================================================
2025-12-04 10:30:40,276 - __main__ - INFO - 
2025-12-04 10:30:40,276 - __main__ - INFO - STEP 1: Testing Database Connection
2025-12-04 10:30:40,276 - __main__ - INFO - --------------------------------------------------------------------------------
2025-12-04 10:30:40,655 - __main__ - INFO - 
2025-12-04 10:30:40,655 - __main__ - INFO - STEP 2: EXTRACT PHASE
2025-12-04 10:30:40,655 - __main__ - INFO - --------------------------------------------------------------------------------
2025-12-04 10:30:41,154 - __main__ - INFO - Extracting Sales Data...
2025-12-04 10:30:41,154 - etl.extract.extract_sales - INFO - Starting sales data extraction...
2025-12-04 10:30:41,154 - etl.extract.extract_sales - INFO - Reading file: D:\FILE SM 5\nourish-beauty-dwh-uiux\data\raw\SuperMarket-Analysis-penjualan.csv
2025-12-04 10:30:41,164 - etl.extract.extract_sales - INFO - [OK] Loaded 1000 rows from SuperMarket-Analysis-penjualan.csv
2025-12-04 10:30:41,165 - etl.extract.extract_sales - INFO - Applying transformations...
2025-12-04 10:30:41,165 - etl.transform.transform_sales - INFO - Applying 40 transformation rules to sales data...
2025-12-04 10:30:41,165 - etl.transform.transform_sales - INFO -    Initial row count: 1000
2025-12-04 10:30:41,165 - etl.transform.transform_sales - INFO - RULE 0: Pre-cleaning Indonesian number format...
2025-12-04 10:30:41,167 - etl.transform.transform_sales - INFO -    Fixed persentase_gross_margin scale
2025-12-04 10:30:41,167 - etl.transform.transform_sales - INFO - RULE 1-5: Data Type Conversion
2025-12-04 10:30:41,167 - etl.transform.transform_sales - INFO - RULE 6-10: Handle Missing Values
2025-12-04 10:30:41,176 - etl.transform.transform_sales - INFO - RULE 11-15: Data Cleaning
2025-12-04 10:30:41,182 - etl.transform.transform_sales - INFO - RULE 16-20: Standardization
2025-12-04 10:30:41,186 - etl.transform.transform_sales - INFO - RULE 21-25: Calculate Derived Fields
2025-12-04 10:30:41,190 - etl.transform.transform_sales - INFO - RULE 26-30: Validation & Quality Checks
2025-12-04 10:30:41,192 - etl.transform.transform_sales - INFO - RULE 31-35: Business Logic & Categorization
2025-12-04 10:30:41,203 - etl.transform.transform_sales - INFO - RULE 36-40: Final Cleaning & Outlier Removal
2025-12-04 10:30:41,209 - etl.transform.transform_sales - INFO - [OK] Transformation complete!
2025-12-04 10:30:41,209 - etl.transform.transform_sales - INFO -    Initial rows: 1000
2025-12-04 10:30:41,209 - etl.transform.transform_sales - INFO -    Final rows: 991
2025-12-04 10:30:41,209 - etl.transform.transform_sales - INFO -    Removed: 9 (0.9%)
2025-12-04 10:30:41,209 - etl.transform.transform_sales - INFO -    Retention rate: 99.1%
2025-12-04 10:30:41,236 - etl.extract.extract_sales - INFO - [OK] Saved to staging file: D:\FILE SM 5\nourish-beauty-dwh-uiux\data\staging\staging_sales.csv
2025-12-04 10:30:41,236 - etl.extract.extract_sales - INFO - [OK] Extraction complete: 991 records ready
2025-12-04 10:30:41,236 - etl.extract.extract_sales - INFO - Loading sales data to staging_sales table...
2025-12-04 10:30:41,236 - etl.extract.extract_sales - INFO -    Records to load: 991
2025-12-04 10:30:41,236 - etl.extract.extract_sales - INFO -    Truncating staging_sales table...
2025-12-04 10:30:41,327 - etl.extract.extract_sales - INFO -    Loading data in 2 chunks of 500 rows...
2025-12-04 10:30:41,575 - etl.extract.extract_sales - INFO -    Chunk 1/2 loaded (500 rows)
2025-12-04 10:30:41,735 - etl.extract.extract_sales - INFO -    Chunk 2/2 loaded (491 rows)
2025-12-04 10:30:41,735 - etl.extract.extract_sales - INFO - [OK] Successfully loaded 991 rows to staging_sales
2025-12-04 10:30:41,735 - etl.extract.extract_sales - INFO - [OK] Verified staging_sales row count: 991
2025-12-04 10:30:41,735 - __main__ - INFO - Extracting HR Data...
2025-12-04 10:30:41,735 - etl.extract.extract_hr - INFO - Starting HR data extraction...
2025-12-04 10:30:41,735 - etl.extract.extract_hr - INFO - Reading file: D:\FILE SM 5\nourish-beauty-dwh-uiux\data\raw\HRDataset.csv
2025-12-04 10:30:41,752 - etl.extract.extract_hr - INFO - [OK] Loaded 311 rows from HRDataset.csv
2025-12-04 10:30:41,753 - etl.extract.extract_hr - INFO -    Columns: 36
2025-12-04 10:30:41,753 - etl.extract.extract_hr - INFO -    Converting date column: dob
2025-12-04 10:30:41,755 - etl.extract.extract_hr - INFO -    Converting date column: dateofhire
2025-12-04 10:30:41,757 - etl.extract.extract_hr - INFO -    Converting date column: dateoftermination
2025-12-04 10:30:41,759 - etl.extract.extract_hr - INFO -    Converting date column: lastperformancereview_date
2025-12-04 10:30:41,779 - etl.extract.extract_hr - INFO - [OK] Saved to staging file: D:\FILE SM 5\nourish-beauty-dwh-uiux\data\staging\staging_hr.csv
2025-12-04 10:30:41,779 - etl.extract.extract_hr - INFO - [OK] Extraction complete: 311 records ready
2025-12-04 10:30:41,780 - etl.extract.extract_hr - INFO - Loading HR data to staging_hr table...
2025-12-04 10:30:41,780 - etl.extract.extract_hr - INFO -    Records to load: 311
2025-12-04 10:30:41,780 - etl.extract.extract_hr - INFO -    Truncating staging_hr table...
2025-12-04 10:30:41,837 - etl.extract.extract_hr - INFO -    Loading rows individually...
2025-12-04 10:30:42,596 - etl.extract.extract_hr - INFO -    Progress: 50/311 rows loaded
2025-12-04 10:30:43,179 - etl.extract.extract_hr - INFO -    Progress: 100/311 rows loaded
2025-12-04 10:30:43,769 - etl.extract.extract_hr - INFO -    Progress: 150/311 rows loaded
2025-12-04 10:30:44,200 - etl.extract.extract_hr - INFO -    Progress: 200/311 rows loaded
2025-12-04 10:30:44,658 - etl.extract.extract_hr - INFO -    Progress: 250/311 rows loaded
2025-12-04 10:30:45,057 - etl.extract.extract_hr - INFO -    Progress: 300/311 rows loaded
2025-12-04 10:30:45,159 - etl.extract.extract_hr - INFO -    Progress: 311/311 rows loaded
2025-12-04 10:30:45,159 - etl.extract.extract_hr - INFO - [OK] Successfully loaded 311 rows
2025-12-04 10:30:45,159 - etl.extract.extract_hr - INFO - [OK] Verified row count: 311
2025-12-04 10:30:45,159 - __main__ - INFO - Extracting Marketing Data...
2025-12-04 10:30:45,159 - etl.extract.extract_marketing - INFO - Starting marketing data extraction...
2025-12-04 10:30:45,159 - etl.extract.extract_marketing - INFO - Reading file: D:\FILE SM 5\nourish-beauty-dwh-uiux\data\raw\marketing_campaign.csv
2025-12-04 10:30:45,159 - etl.extract.extract_marketing - INFO -    Trying delimiter: '	'
2025-12-04 10:30:45,159 - etl.extract.extract_marketing - WARNING -    Failed: only 1 columns detected
2025-12-04 10:30:45,159 - etl.extract.extract_marketing - INFO -    Trying delimiter: ';'
2025-12-04 10:30:45,192 - etl.extract.extract_marketing - INFO -    [OK] Successfully read with delimiter ';'
2025-12-04 10:30:45,192 - etl.extract.extract_marketing - INFO -    Columns detected: 29
2025-12-04 10:30:45,193 - etl.extract.extract_marketing - INFO - [OK] Loaded 1000 rows from marketing_campaign.csv
2025-12-04 10:30:45,193 - etl.extract.extract_marketing - INFO -    Columns: ['ID', 'Year_Birth', 'Education', 'Marital_Status', 'Income']... (29 total)
2025-12-04 10:30:45,195 - etl.extract.extract_marketing - INFO -    Converting date column: dt_customer
2025-12-04 10:30:45,214 - etl.extract.extract_marketing - INFO - [OK] Saved to staging file: D:\FILE SM 5\nourish-beauty-dwh-uiux\data\staging\staging_marketing.csv
2025-12-04 10:30:45,214 - etl.extract.extract_marketing - INFO - [OK] Extraction complete: 1000 records ready
2025-12-04 10:30:45,214 - etl.extract.extract_marketing - INFO - Loading marketing data to staging_marketing table...
2025-12-04 10:30:45,214 - etl.extract.extract_marketing - INFO -    Records to load: 1000
2025-12-04 10:30:45,214 - etl.extract.extract_marketing - INFO -    Dropping and recreating staging_marketing table...
2025-12-04 10:30:45,371 - etl.extract.extract_marketing - INFO -    Loading data with auto-schema detection...
2025-12-04 10:30:45,767 - etl.extract.extract_marketing - INFO - [OK] Successfully loaded 1000 rows
2025-12-04 10:30:45,767 - etl.extract.extract_marketing - INFO - [OK] Verified row count: 1000
2025-12-04 10:30:45,767 - __main__ - INFO - [OK] Extract phase completed!
2025-12-04 10:30:45,767 - __main__ - INFO - 
2025-12-04 10:30:45,767 - __main__ - INFO - STEP 3: TRANSFORM PHASE
2025-12-04 10:30:45,767 - __main__ - INFO - --------------------------------------------------------------------------------
2025-12-04 10:30:45,767 - __main__ - INFO - Transformation rules will be applied during load phase
2025-12-04 10:30:45,767 - __main__ - INFO - [OK] 40 transformation rules ready
2025-12-04 10:30:45,767 - __main__ - INFO - 
2025-12-04 10:30:45,767 - __main__ - INFO - STEP 4: LOAD DIMENSIONS
2025-12-04 10:30:45,767 - __main__ - INFO - --------------------------------------------------------------------------------
2025-12-04 10:30:45,801 - etl.load.load_dimensions - INFO - ======================================================================
2025-12-04 10:30:45,801 - etl.load.load_dimensions - INFO - STARTING DIMENSION LOAD PROCESS
2025-12-04 10:30:45,801 - etl.load.load_dimensions - INFO - ======================================================================
2025-12-04 10:30:45,802 - etl.load.load_dimensions - INFO - Loading dim_produk...
2025-12-04 10:30:45,994 - etl.load.load_dimensions - INFO - [OK] Loaded 6 products to dim_produk
2025-12-04 10:30:45,994 - etl.load.load_dimensions - INFO - Loading dim_customer...
2025-12-04 10:30:46,054 - etl.load.load_dimensions - INFO -    Truncated dim_customer table
2025-12-04 10:30:46,103 - etl.load.load_dimensions - INFO -    Loaded chunk 1: 50 rows (Total: 50/1000)
2025-12-04 10:30:46,117 - etl.load.load_dimensions - INFO -    Loaded chunk 2: 50 rows (Total: 100/1000)
2025-12-04 10:30:46,135 - etl.load.load_dimensions - INFO -    Loaded chunk 3: 50 rows (Total: 150/1000)
2025-12-04 10:30:46,151 - etl.load.load_dimensions - INFO -    Loaded chunk 4: 50 rows (Total: 200/1000)
2025-12-04 10:30:46,166 - etl.load.load_dimensions - INFO -    Loaded chunk 5: 50 rows (Total: 250/1000)
2025-12-04 10:30:46,182 - etl.load.load_dimensions - INFO -    Loaded chunk 6: 50 rows (Total: 300/1000)
2025-12-04 10:30:46,196 - etl.load.load_dimensions - INFO -    Loaded chunk 7: 50 rows (Total: 350/1000)
2025-12-04 10:30:46,214 - etl.load.load_dimensions - INFO -    Loaded chunk 8: 50 rows (Total: 400/1000)
2025-12-04 10:30:46,230 - etl.load.load_dimensions - INFO -    Loaded chunk 9: 50 rows (Total: 450/1000)
2025-12-04 10:30:46,242 - etl.load.load_dimensions - INFO -    Loaded chunk 10: 50 rows (Total: 500/1000)
2025-12-04 10:30:46,262 - etl.load.load_dimensions - INFO -    Loaded chunk 11: 50 rows (Total: 550/1000)
2025-12-04 10:30:46,278 - etl.load.load_dimensions - INFO -    Loaded chunk 12: 50 rows (Total: 600/1000)
2025-12-04 10:30:46,294 - etl.load.load_dimensions - INFO -    Loaded chunk 13: 50 rows (Total: 650/1000)
2025-12-04 10:30:46,311 - etl.load.load_dimensions - INFO -    Loaded chunk 14: 50 rows (Total: 700/1000)
2025-12-04 10:30:46,327 - etl.load.load_dimensions - INFO -    Loaded chunk 15: 50 rows (Total: 750/1000)
2025-12-04 10:30:46,347 - etl.load.load_dimensions - INFO -    Loaded chunk 16: 50 rows (Total: 800/1000)
2025-12-04 10:30:46,362 - etl.load.load_dimensions - INFO -    Loaded chunk 17: 50 rows (Total: 850/1000)
2025-12-04 10:30:46,374 - etl.load.load_dimensions - INFO -    Loaded chunk 18: 50 rows (Total: 900/1000)
2025-12-04 10:30:46,388 - etl.load.load_dimensions - INFO -    Loaded chunk 19: 50 rows (Total: 950/1000)
2025-12-04 10:30:46,407 - etl.load.load_dimensions - INFO -    Loaded chunk 20: 50 rows (Total: 1000/1000)
2025-12-04 10:30:46,409 - etl.load.load_dimensions - INFO - [OK] Successfully loaded 1000 customers to dim_customer
2025-12-04 10:30:46,410 - etl.load.load_dimensions - INFO - Loading dim_employee...
2025-12-04 10:30:46,535 - etl.load.load_dimensions - INFO - [OK] Loaded 311 employees to dim_employee
2025-12-04 10:30:46,535 - etl.load.load_dimensions - INFO - Verifying dimension tables...
2025-12-04 10:30:46,594 - etl.load.load_dimensions - INFO - ======================================================================
2025-12-04 10:30:46,594 - etl.load.load_dimensions - INFO - [OK] ALL DIMENSIONS LOADED SUCCESSFULLY!
2025-12-04 10:30:46,595 - etl.load.load_dimensions - INFO - Total time: 0.79 seconds
2025-12-04 10:30:46,595 - etl.load.load_dimensions - INFO - ======================================================================
2025-12-04 10:30:46,595 - __main__ - INFO - 
2025-12-04 10:30:46,595 - __main__ - INFO - STEP 5: LOAD FACT TABLES
2025-12-04 10:30:46,595 - __main__ - INFO - --------------------------------------------------------------------------------
2025-12-04 10:30:46,600 - etl.load.load_facts - INFO - ======================================================================
2025-12-04 10:30:46,600 - etl.load.load_facts - INFO - STARTING FACT TABLES LOAD PROCESS
2025-12-04 10:30:46,600 - etl.load.load_facts - INFO - ======================================================================
2025-12-04 10:30:46,600 - etl.load.load_facts - INFO - Loading fact_sales...
2025-12-04 10:30:46,691 - etl.load.load_facts - INFO - [OK] Loaded 664 rows to fact_sales
2025-12-04 10:30:46,691 - etl.load.load_facts - INFO - Loading fact_marketing_response...
2025-12-04 10:30:46,756 - etl.load.load_facts - INFO - [OK] Loaded 0 rows to fact_marketing_response
2025-12-04 10:30:46,756 - etl.load.load_facts - INFO - Loading fact_employee_performance...
2025-12-04 10:30:46,816 - etl.load.load_facts - ERROR - [ERROR] Error loading fact_employee_performance: (psycopg2.errors.UndefinedColumn) column h.emp_id does not exist
LINE 25:         INNER JOIN dim_employee e ON h.emp_id = e.emp_id
                                              ^
HINT:  Perhaps you meant to reference the column "h.empid" or the column "e.emp_id".

[SQL: 
        INSERT INTO fact_employee_performance (
            tanggal_key, employee_key,
            performance_score_id, performance_score,
            engagement_survey, emp_satisfaction,
            special_projects_count, days_late_last_30,
            absences, salary, recruitment_source,
            review_date, created_date
        )
        SELECT 
            dt.tanggal_key,
            e.employee_key,
            h.perf_score_id,
            h.performance_score,
            h.engagement_survey,
            h.emp_satisfaction,
            h.special_projects_count,
            h.days_late_last_30,
            h.absences,
            h.salary,
            h.recruitment_source,
            h.last_performance_review_date,
            NOW()
        FROM staging_hr h
        INNER JOIN dim_employee e ON h.emp_id = e.emp_id
        INNER JOIN dim_tanggal dt ON h.last_performance_review_date = dt.tanggal
        WHERE h.emp_id IS NOT NULL
        AND h.last_performance_review_date IS NOT NULL
        ON CONFLICT DO NOTHING;
        ]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-12-04 10:30:46,819 - etl.load.load_facts - ERROR - Traceback (most recent call last):
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\default.py", line 951, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column h.emp_id does not exist
LINE 25:         INNER JOIN dim_employee e ON h.emp_id = e.emp_id
                                              ^
HINT:  Perhaps you meant to reference the column "h.empid" or the column "e.emp_id".


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\FILE SM 5\nourish-beauty-dwh-uiux\etl\load\load_facts.py", line 234, in load_fact_employee_performance
    result = conn.execute(query)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py", line 1419, in execute
    return meth(
           ^^^^^
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\sql\elements.py", line 526, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py", line 1846, in _execute_context
    return self._exec_single_context(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py", line 2355, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\default.py", line 951, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column h.emp_id does not exist
LINE 25:         INNER JOIN dim_employee e ON h.emp_id = e.emp_id
                                              ^
HINT:  Perhaps you meant to reference the column "h.empid" or the column "e.emp_id".

[SQL: 
        INSERT INTO fact_employee_performance (
            tanggal_key, employee_key,
            performance_score_id, performance_score,
            engagement_survey, emp_satisfaction,
            special_projects_count, days_late_last_30,
            absences, salary, recruitment_source,
            review_date, created_date
        )
        SELECT 
            dt.tanggal_key,
            e.employee_key,
            h.perf_score_id,
            h.performance_score,
            h.engagement_survey,
            h.emp_satisfaction,
            h.special_projects_count,
            h.days_late_last_30,
            h.absences,
            h.salary,
            h.recruitment_source,
            h.last_performance_review_date,
            NOW()
        FROM staging_hr h
        INNER JOIN dim_employee e ON h.emp_id = e.emp_id
        INNER JOIN dim_tanggal dt ON h.last_performance_review_date = dt.tanggal
        WHERE h.emp_id IS NOT NULL
        AND h.last_performance_review_date IS NOT NULL
        ON CONFLICT DO NOTHING;
        ]
(Background on this error at: https://sqlalche.me/e/20/f405)

2025-12-04 10:30:46,822 - etl.load.load_facts - ERROR - ======================================================================
2025-12-04 10:30:46,822 - etl.load.load_facts - ERROR - [ERROR] FACT LOAD FAILED: (psycopg2.errors.UndefinedColumn) column h.emp_id does not exist
LINE 25:         INNER JOIN dim_employee e ON h.emp_id = e.emp_id
                                              ^
HINT:  Perhaps you meant to reference the column "h.empid" or the column "e.emp_id".

[SQL: 
        INSERT INTO fact_employee_performance (
            tanggal_key, employee_key,
            performance_score_id, performance_score,
            engagement_survey, emp_satisfaction,
            special_projects_count, days_late_last_30,
            absences, salary, recruitment_source,
            review_date, created_date
        )
        SELECT 
            dt.tanggal_key,
            e.employee_key,
            h.perf_score_id,
            h.performance_score,
            h.engagement_survey,
            h.emp_satisfaction,
            h.special_projects_count,
            h.days_late_last_30,
            h.absences,
            h.salary,
            h.recruitment_source,
            h.last_performance_review_date,
            NOW()
        FROM staging_hr h
        INNER JOIN dim_employee e ON h.emp_id = e.emp_id
        INNER JOIN dim_tanggal dt ON h.last_performance_review_date = dt.tanggal
        WHERE h.emp_id IS NOT NULL
        AND h.last_performance_review_date IS NOT NULL
        ON CONFLICT DO NOTHING;
        ]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-12-04 10:30:46,823 - etl.load.load_facts - ERROR - ======================================================================
2025-12-04 10:30:46,824 - etl.load.load_facts - ERROR - Traceback (most recent call last):
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\default.py", line 951, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column h.emp_id does not exist
LINE 25:         INNER JOIN dim_employee e ON h.emp_id = e.emp_id
                                              ^
HINT:  Perhaps you meant to reference the column "h.empid" or the column "e.emp_id".


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\FILE SM 5\nourish-beauty-dwh-uiux\etl\load\load_facts.py", line 307, in load_all_facts
    load_fact_employee_performance()
  File "D:\FILE SM 5\nourish-beauty-dwh-uiux\etl\load\load_facts.py", line 234, in load_fact_employee_performance
    result = conn.execute(query)
             ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py", line 1419, in execute
    return meth(
           ^^^^^
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\sql\elements.py", line 526, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py", line 1846, in _execute_context
    return self._exec_single_context(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py", line 2355, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\default.py", line 951, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column h.emp_id does not exist
LINE 25:         INNER JOIN dim_employee e ON h.emp_id = e.emp_id
                                              ^
HINT:  Perhaps you meant to reference the column "h.empid" or the column "e.emp_id".

[SQL: 
        INSERT INTO fact_employee_performance (
            tanggal_key, employee_key,
            performance_score_id, performance_score,
            engagement_survey, emp_satisfaction,
            special_projects_count, days_late_last_30,
            absences, salary, recruitment_source,
            review_date, created_date
        )
        SELECT 
            dt.tanggal_key,
            e.employee_key,
            h.perf_score_id,
            h.performance_score,
            h.engagement_survey,
            h.emp_satisfaction,
            h.special_projects_count,
            h.days_late_last_30,
            h.absences,
            h.salary,
            h.recruitment_source,
            h.last_performance_review_date,
            NOW()
        FROM staging_hr h
        INNER JOIN dim_employee e ON h.emp_id = e.emp_id
        INNER JOIN dim_tanggal dt ON h.last_performance_review_date = dt.tanggal
        WHERE h.emp_id IS NOT NULL
        AND h.last_performance_review_date IS NOT NULL
        ON CONFLICT DO NOTHING;
        ]
(Background on this error at: https://sqlalche.me/e/20/f405)

2025-12-04 10:30:46,827 - __main__ - INFO - 
2025-12-04 10:30:46,827 - __main__ - INFO - STEP 6: DATA VERIFICATION
2025-12-04 10:30:46,827 - __main__ - INFO - --------------------------------------------------------------------------------
2025-12-04 10:30:46,874 - __main__ - INFO - 
Data Warehouse Row Counts:
2025-12-04 10:30:46,874 - __main__ - INFO - 
               table_name  row_count
               dim_cabang          6
             dim_customer       1000
             dim_employee        311
              dim_payment          6
               dim_produk          6
              dim_tanggal       4748
fact_employee_performance          0
  fact_marketing_response          0
               fact_sales        664
2025-12-04 10:30:46,874 - __main__ - INFO - 
2025-12-04 10:30:46,874 - __main__ - INFO - ================================================================================
2025-12-04 10:30:46,874 - __main__ - INFO - [OK] ETL PIPELINE COMPLETED SUCCESSFULLY!
2025-12-04 10:30:46,874 - __main__ - INFO - ================================================================================
2025-12-04 10:30:46,889 - __main__ - INFO - Log file: logs\etl_pipeline_20251204_103040.log
2025-12-04 10:30:46,889 - __main__ - INFO - Duration: 6.60 seconds
2025-12-04 10:30:46,889 - __main__ - INFO - Completed at: 2025-12-04 10:30:46
2025-12-04 10:30:46,889 - __main__ - INFO - ================================================================================
